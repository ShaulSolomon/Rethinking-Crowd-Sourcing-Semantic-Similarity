{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bitvenvsotavenvd1cd1d8c20304d169b1d1dfb67d3e4a1",
   "display_name": "Python 3.6.9 64-bit ('venv_sota': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from transformers import DistilBertModel,DistilBertTokenizer\n",
    "from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "/home/shaul/workspace/GitHub/SOTA\n"
    }
   ],
   "source": [
    "PATH_ROOT = \"/home/shaul/workspace/GitHub/SOTA/\"\n",
    "%cd {PATH_ROOT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/combined/combined_dataset.csv\").drop(['Unnamed: 0'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "        annotator                                              text1  \\\n0  A3BCGN11HDM8QR  And he sent Eliakim , who was over the househo...   \n1  A3SQ00HYQN7FYB  And he sent Eliakim , who was over the househo...   \n2    A5WAWW70PYRP  And he sent Eliakim , who was over the househo...   \n\n                                               text2  label      dataset  \\\n0  And he sent Eliakim , who was over the house ,...      2  bible_human   \n1  And he sent Eliakim , who was over the house ,...      3  bible_human   \n2  And he sent Eliakim , who was over the house ,...      4  bible_human   \n\n   random                   duration  total_seconds pair_id  reduced_label  \n0       0  0 days 00:00:12.000000000             12  pair_0             -1  \n1       0  0 days 00:00:12.000000000             12  pair_0              0  \n2       0  0 days 00:07:19.000000000            439  pair_0              1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>annotator</th>\n      <th>text1</th>\n      <th>text2</th>\n      <th>label</th>\n      <th>dataset</th>\n      <th>random</th>\n      <th>duration</th>\n      <th>total_seconds</th>\n      <th>pair_id</th>\n      <th>reduced_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A3BCGN11HDM8QR</td>\n      <td>And he sent Eliakim , who was over the househo...</td>\n      <td>And he sent Eliakim , who was over the house ,...</td>\n      <td>2</td>\n      <td>bible_human</td>\n      <td>0</td>\n      <td>0 days 00:00:12.000000000</td>\n      <td>12</td>\n      <td>pair_0</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A3SQ00HYQN7FYB</td>\n      <td>And he sent Eliakim , who was over the househo...</td>\n      <td>And he sent Eliakim , who was over the house ,...</td>\n      <td>3</td>\n      <td>bible_human</td>\n      <td>0</td>\n      <td>0 days 00:00:12.000000000</td>\n      <td>12</td>\n      <td>pair_0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A5WAWW70PYRP</td>\n      <td>And he sent Eliakim , who was over the househo...</td>\n      <td>And he sent Eliakim , who was over the house ,...</td>\n      <td>4</td>\n      <td>bible_human</td>\n      <td>0</td>\n      <td>0 days 00:07:19.000000000</td>\n      <td>439</td>\n      <td>pair_0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Removing Outliers\n",
    "# \n",
    "# Under the assumption that by removing some of the bad actors in the dataset, the metrics might be more reflective of the human labeling, we want to currently:\n",
    "# \n",
    "# 1. Find out who the bad actors are\n",
    "# 2. See if there is any overlap from the dif. category of bad actors\n",
    "# \n",
    "# (Removing bad phrases should also be done - but is beyond the scope of this notebook)\n",
    "# \n",
    "# \n",
    "# The categories of bad actors are:\n",
    "# \n",
    "# [x] - Slow Annotators - ba_time </br>\n",
    "# [x] - Greater variance in random datasets than non-random datasets - ba_unvar_annotations </br>\n",
    "# [x] - Unpopular (disagree with two others often) - ba_unpopular </br>\n",
    "# [ ] - Inconsistent with sentiment != semantics - ba_semantics </br>"
   ]
  },
  {
   "source": [
    "# # Removing Outliers\n",
    "# \n",
    "# Under the assumption that by removing some of the bad actors in the dataset, the metrics might be more reflective of the human labeling,we want to currently:\n",
    " \n",
    " 1. Find out who the bad actors are\n",
    " 2. See if there is any overlap from the dif. category of bad actors\n",
    " \n",
    " (Removing bad phrases should also be done - but is beyond the scope of this notebook)\n",
    " \n",
    " \n",
    " The categories of bad actors are:\n",
    " \n",
    " [x] - Slow Annotators </br>\n",
    " [x] - Greater variance in random datasets than non-random datasets </br>\n",
    "[ ] - Unpopular (disagree with two others often) </br>\n",
    " [ ] - Inconsistent with sentiment != semantics </br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Removing Outliers\n",
    " \n",
    "Under the assumption that by removing some of the bad actors in the dataset, the metrics might be more reflective of the human labeling,we want to currently:\n",
    " \n",
    " 1. Find out who the bad actors are\n",
    " 2. See if there is any overlap from the dif. category of bad actors\n",
    " \n",
    " (Removing bad phrases should also be done - but is beyond the scope of this notebook)\n",
    " \n",
    " \n",
    " The categories of bad actors are:\n",
    " \n",
    " [x] - Slow Annotators </br>\n",
    " [x] - Greater variance in random datasets than non-random datasets </br>\n",
    " [ ] - Unpopular (disagree with two others often) </br>\n",
    " [ ] - Inconsistent with sentiment != semantics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Removing Outliers\n",
    " \n",
    "Under the assumption that by removing some of the bad actors in the dataset, the metrics might be more reflective of the human labeling,we want to currently:\n",
    " \n",
    " 1. Find out who the bad actors are\n",
    " 2. See if there is any overlap from the dif. category of bad actors\n",
    " \n",
    " (Removing bad phrases should also be done - but is beyond the scope of this notebook)\n",
    " \n",
    " \n",
    " The categories of bad actors are:\n",
    " \n",
    " [x] - Slow Annotators </br>\n",
    " [x] - Greater variance in random datasets than non-random datasets </br>\n",
    " [ ] - Unpopular (disagree with two others often) </br>\n",
    " [ ] - Inconsistent with sentiment != semantics </br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Removing Outliers\n",
    "\n",
    "Under the assumption that by removing some of the bad actors in the dataset, the metrics might be more reflective of the human labeling, we want to currently:\n",
    "\n",
    "1. Find out who the bad actors are\n",
    "2. See if there is any overlap from the dif. category of bad actors\n",
    "\n",
    "(Removing bad phrases should also be done - but is beyond the scope of this notebook)\n",
    "\n",
    "\n",
    "The categories of bad actors are:\n",
    "\n",
    "[x] - Slow Annotators </br>\n",
    "[x] - Greater variance in random datasets than non-random datasets </br>\n",
    "[ ] - Unpopular (disagree with two others often) </br>\n",
    "[ ] - Inconsistent with sentiment != semantics </br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Time Outliers\n",
    "\n",
    "Under the assumption that anyone that takes over the 95 percentile of time."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "count    39660.000000\nmean        86.010867\nstd        427.060791\nmin          1.000000\n25%          7.000000\n50%         12.000000\n75%         28.000000\n90%        169.000000\n95%        336.000000\nmax      14079.000000\nName: total_seconds, dtype: float64\n"
    }
   ],
   "source": [
    "print(df.total_seconds.describe(percentiles = [.25,.5,.75,.9,.95]))\n",
    "\n",
    "# ba = bad actor\n",
    "ba_time = list(df[df.total_seconds > 336].annotator)"
   ]
  },
  {
   "source": [
    "## Unvarianced Annotations\n",
    "Labelers whos std is too low mean non-random - random difference is too high  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelers = df[df.random==0].groupby(['annotator'])['label'].agg(['size','mean','std','min','max'])\n",
    "labelers = labelers[labelers['size']>1]\n",
    "#df = df[df.annotator.apply(lambda x:x in set(labelers.index))]\n",
    "\n",
    "labelers_rand = df[df.random==1].groupby(['annotator'])['label'].agg(['size','mean','std','min','max'])\n",
    "labelers_rand = labelers_rand[labelers_rand['size']>1]\n",
    "labelers = labelers.join(labelers_rand, rsuffix = '_rand')\n",
    "labelers['mean_random_gap'] = labelers['mean']-labelers['mean_rand']\n",
    "labelers['std_ratio'] = labelers['std']/labelers['std_rand']\n",
    "\n",
    "total_std = df.groupby('annotator')['label'].std()\n",
    "total_std.name = 'total_std'\n",
    "labelers = labelers.join(total_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ba_unvar_annotations = list(labelers[(labelers.total_std<1) & (labelers.mean_random_gap < 0)].index)"
   ]
  },
  {
   "source": [
    "## Unpopular Annotators\n",
    "Those who over 50% of the time, disagree with the other annotators (in the reduced label)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uniquelabels = df.groupby(\"pair_id\")[\"reduced_label\"].nunique()\n",
    "pairs_twoagree = list(df_uniquelabels[(df.groupby(\"pair_id\")[\"reduced_label\"].nunique() == 2).values].index)\n",
    "df_twoagree = df[df[\"pair_id\"].isin(pairs_twoagree)]\n",
    "\n",
    "df_id_reducedlabel = df_twoagree.groupby(\"pair_id\")['reduced_label'].median()\n",
    "df_twoagree['generally_accepted_label']  = df_id_reducedlabel.values.repeat(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unpopularopinion = df_twoagree[df_twoagree.reduced_label != df_twoagree.generally_accepted_label].groupby('annotator').size().reset_index()\n",
    "df_unpopularopinion.columns = ['annotator','unpopular_opinion']\n",
    "\n",
    "df_allopinions = df[df['annotator'].isin(list(df_unpopularopinion.annotator))].groupby('annotator').size().reset_index()\n",
    "df_allopinions.columns = ['annotator','all_opinion']\n",
    "\n",
    "df_opinion_all_unpop = df_allopinions.merge(df_unpopularopinion,on=\"annotator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ba_unpopular = list(df_opinion_all_unpop[((df_opinion_all_unpop.unpopular_opinion / df_opinion_all_unpop.all_opinion) > 0.5) & (df_opinion_all_unpop.all_opinion > 4)].annotator)"
   ]
  },
  {
   "source": [
    "## Sentiment // Semantic Understanding"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment analysis pipeline\n",
    "sentiment_pipe = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 397/397 [19:35<00:00,  2.96s/it]\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "39660"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "text1_sent,text2_sent =[], []\n",
    "\n",
    "pbar = tqdm(total = len(df)//100+1, position = 0, leave = True)\n",
    "for i in range (len(df)//100+1):\n",
    "    t1_s = sentiment_pipe(df.text1.tolist()[100*i:np.min([100*i+100,len(df)])])\n",
    "    t2_s = sentiment_pipe(df.text2.tolist()[100*i:np.min([100*i+100,len(df)])])\n",
    "    text1_sent+=t1_s\n",
    "    text2_sent+=t2_s\n",
    "    pbar.update()\n",
    "pbar.close()\n",
    "len(text1_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = {'POSITIVE':1,'NEGATIVE':-1}\n",
    "\n",
    "df['sentiment_1'] = np.array([x['score']*sent[x['label']] for x in text1_sent]) \n",
    "df['sentiment_2'] = np.array([x['score']*sent[x['label']] for x in text2_sent])\n",
    "df['dif_sent'] =  np.abs(df['sentiment_1']-df['sentiment_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = []\n",
    "for index, row in df.iterrows():\n",
    "    first_sentence_tokens = row['text1'].strip().split()\n",
    "    second_sentence_tokens = row['text2'].strip().split()\n",
    "    pairs.append((first_sentence_tokens, second_sentence_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.305811962880319\n0.2615081970765116\n"
    }
   ],
   "source": [
    "scores_bleu1 = []\n",
    "for first_sentence_tokens, second_sentence_tokens in pairs:\n",
    "\n",
    "    score_bleu1 = sentence_bleu([first_sentence_tokens], second_sentence_tokens, weights=(1, 0, 0, 0))\n",
    "    scores_bleu1.append(score_bleu1)\n",
    "\n",
    "print(np.mean(scores_bleu1))\n",
    "print(np.std(scores_bleu1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bleu_score_1'] = scores_bleu1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_std_semantic = df[(df['bleu_score_1'] > 0.8) & (df['dif_sent'] > 1.9)].groupby('annotator')['label'].std().dropna()\n",
    "ba_semantics = list(annot_std_semantic[annot_std_semantic > 1.0].index)"
   ]
  },
  {
   "source": [
    "# Combining all the results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(list1, list2):\n",
    "    s1 = set(list1)\n",
    "    s2 = set(list2)\n",
    "    return len(s1.intersection(s2)) / len(s1.union(s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Jaccard Similarity of ba_semantics and ba_time is :0.096\nJaccard Similarity of ba_semantics and ba_unpopular is :0.027777777777777776\nJaccard Similarity of ba_semantics and ba_unvar_annotations is :0.045454545454545456\nJaccard Similarity of ba_time and ba_unpopular is :0.12142857142857143\nJaccard Similarity of ba_time and ba_unvar_annotations is :0.08403361344537816\nJaccard Similarity of ba_unpopular and ba_unvar_annotations is :0.15789473684210525\n"
    }
   ],
   "source": [
    "all_ba = ['ba_semantics','ba_time','ba_unpopular','ba_unvar_annotations']\n",
    "\n",
    "for a,b in list(itertools.combinations(all_ba,2)):\n",
    "    print(f\"Jaccard Similarity of {a} and {b} is :{jaccard_similarity(eval(a),eval(b))}\")"
   ]
  },
  {
   "source": [
    "While the overlap isn't super consistent, it is interesting to note that the two most correlated groups are time and unpopularity and unpopularity with unvaried annotations."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Total number of bad annotators are: 160\nPercentage of total annotators are: 0.34557235421166305\n"
    }
   ],
   "source": [
    "all_ba = list(set(ba_unvar_annotations + ba_unpopular + ba_time + ba_semantics))\n",
    "print(f\"Total number of bad annotators are: {len(all_ba)}\")\n",
    "print(f\"Percentage of total annotators are: {len(all_ba)/df.annotator.nunique()}\")"
   ]
  },
  {
   "source": [
    "### Save the annotators so we can filter them out quicker later"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ba in all_ba:\n",
    "    with open(f'data/other/{ba}.txt', 'w') as f:\n",
    "        for item in eval(ba):\n",
    "            f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/other/ba_all.txt','w') as f:\n",
    "    for item in list(set(ba_unvar_annotations + ba_unpopular + ba_time + ba_semantics)):\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  }
 ]
}