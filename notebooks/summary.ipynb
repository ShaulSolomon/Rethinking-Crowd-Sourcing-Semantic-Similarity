{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"summary.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5-final"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/Hernanros/SOTA/blob/master/summary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"2pRxFGlbGXgt"},"source":["# Rethinking Semantic Similarity Metrics\n","### Cohn, Hershkovitz, Rosenblum, Serfaty, Solomon (Ydata) - 2020\n","##### data files and code can be found in this [repository] (https://github.com/Hernanros/SOTA)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ibd7nRN9GcHG"},"source":["As elaborated in the seminal work (Yamshchikov 20) there is currently no good metric available for semantic similarity, which is  a major obstacle to progress in style transfer and paragraph summarization. Currently an ensemble method (of WMD, BLEU and POS) works best to measure semantic preservation, but even then “[t]here is still no metric that could distinguish paraphrases from style transfers definitively.” While the main focus seems to be on using larger and larger LMs (BERT) for richer sentence embeddings, we propose that an effective solution will only exist when we start “climbing the right hill, not just the hill on whose slope we currently sit…” (Bender 2020)\n","\n","\n","As the cheshire cat said so succinctly, “if you don’t know where you are going, any road will take you there.” While we struggle to find a viable metric for semantic preservation, more fundamental questions must first be addressed:\n","\n","1. What does ‘Semantic Similarity’ even refer to? Besides for the larger epistemological problem of “What does it mean to have meaning?” even humans have an incredibly hard time parsing intent without a larger context.<br><br>\n","2. As we don’t have a clear definition for ourselves what the question even is, on what basis are we even evaluating ‘human-labeled metrics’? There is no clear intuition as to what value, out of five, comparing *“The tiramisu was simply divine.”* and *“Trump once again confuses son-in-law Kushner with oversized elmo doll.”* should give us. As such, why are we even using human labeling as some form of guiding star?<br><br>\n","3. As we don’t have a clear intuition as to why the metrics work at all, if we use an ensemble method, how do we weigh them? Similarly, treating the established metrics as “black box” methods of extracting results, why should we trust any of them at all?<br><br>\n","4. Unlike image recognition, where the information of what is in the picture is found within the pixels of the image, communicative intents are about something that is outside of language. Any model that doesn’t incorporate some form of larger knowledge base will never be able to “understand” anything.<br><br>\n","\n","Before you get too excited, we do not intend to solve any of these problems. Rather, we would like to propose that we would not need to solve any of them fully in order to have a better defined metric.\n","\n","First of, what we are trying to optimize here is not actually semantic similarity, but **perceived** semantic similarity. Unlike other ML tasks (classification, regression, etc.) where we are trying to approximate some real-world distribution, in our situation the goal is to model the intuition people feel for semantic preservation.\n","\n","Trying to tackle perceived semantic similarity, we are no longer trying to define understanding itself, but rather the heuristics humans use when making comparisons. As such, human-labeling is the end-goal. However, due to the fact that subjective evaluation is, by its very definition, subjective, a rigorous approach is to discover and find the effective metrics to the underlying heuristics.\n","\n","**Our hypothesis is that there are several heuristics underlying our conception of perceived semantic similarity, and through exploring and creating metrics that evaluate those underlying heuristics, we can eventually come to a fair evaluation of semantic similarity.** The current metrics in use represent some of the underlying heuristics, which is why they have some effectiveness, but we are not sure as to which heuristics they are mapping onto and to what degree of complexity.\n","\n","Therefore, our goal here is to explore two areas:<br>\n","1. The practical element - if we can assume that the current metrics do capture some of the underlying heuristics, how much can we trust our current metrics (ensemble or otherwise) for the various datasets and how versatile are these ensemble methods if wanted to transfer them to new datasets?<br>\n","\n","2. The theoretical element - We propose the beginning steps (what would start off initially as a social experiment) to explore the underlying heuristics to perceived semantic preservation.\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"FJ1xnKWGLyzi"},"source":["## The Practical element"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"IDF3n0U2GO_Z"},"source":["### Intializations"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"TQMrGCDHDmfn","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","import os, urllib, glob, sys\n","from getpass import getpass\n","\n","import plotly.graph_objects as go\n","from plotly.subplots import make_subplots\n","import matplotlib.pyplot as plt\n","\n","import re\n","\n","from scipy import stats"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5-g706yHDnez","tags":[],"colab":{}},"source":["user = input('User name: ')\n","password = getpass('Password: ')\n","password = urllib.parse.quote(password) # your password is converted into url format\n","cmd_string = \"! git clone https://{0}:{1}@github.com/Hernanros/SOTA\".format(user, password)\n","\n","os.system(cmd_string)\n","cmd_string, password = \"\", \"\" # removing the password from the variable\n","\n","%cd SOTA/data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"T1Q6OCjWNCvw","colab":{}},"source":["root = !pwd\n","root = root[0]\n","# Paraphrase dataset with predictions based off linear and non-linear models\n","df_para = pd.read_csv(f\"{root}/data/Paraphrase_labeled_data_with_predictions_both.csv\").drop(columns=[\"Unnamed: 0\"])\n","\n","# Base dataset with texts and labels\n","df_texts = pd.read_csv(f\"{root}/data/Paraphrase.csv\")\n","\n","# All datasets with predictions based off linear and non-linear models\n","df_all = pd.read_csv(f\"{root}/data/combined_data_with_predictions_on_separate_datasets_both.csv\")\n","\n","# Loss for the MLP model on all datasets\n","df_mlp_testloss = pd.read_csv(f\"{root}/data/test_loss_on_MLP.csv\",names = ['Dataset','Test Loss'],header=0)\n","\n","df_linear_testloss = pd.read_csv(f\"{root}/data/linear_datasets_loss.csv\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"6blhI6zHDpMp"},"source":["### Analysis of Paraphrase Dataset"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"427gwtKsHKRd"},"source":["We initially started with the Paraphrase dataset and processed the scores for all of the following metrics (normalized with MinMax Scaler [0,5] for easier readability):"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ZgrDy1_oMvSG","colab":{}},"source":["df_para.head(3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"2tmC4J63R6rm"},"source":["We then trained a linear model (RF) \n","\n","```\n","model = RandomForestRegressor(max_depth=2)\n","```\n","\n","and explored the weights:"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"gV6XrGBLR-yM"},"source":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th></th>\n","      <th>feature</th>\n","      <th>importance</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>3</th>\n","      <td>WMD</td>\n","      <td>0.376846</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>chrf_score_norm</td>\n","      <td>0.248155</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>BertScore</td>\n","      <td>0.195598</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>L2_score</td>\n","      <td>0.087413</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>ROUGE-1 precision</td>\n","      <td>0.021447</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>ROUGE-L F</td>\n","      <td>0.021338</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>ROUGE-L precision</td>\n","      <td>0.017681</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1-gram_overlap</td>\n","      <td>0.010945</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>ROUGE-2 recall</td>\n","      <td>0.009196</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>ROUGE-L recall</td>\n","      <td>0.004518</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>ROUGE-1 F</td>\n","      <td>0.003775</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ROUGE-1 recall</td>\n","      <td>0.003087</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>POS dist score</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>ROUGE-2 precision</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>ROUGE-2 F</td>\n","      <td>0.000000</td>\n","    </tr>\n","  </tbody>\n","</table>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"vwvjf3TDS5S8"},"source":["and the test loss (MAE):  0.6027"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"EuKCtXp1NE8Z"},"source":["\n","\n","and a simple MLP non-linear model\n","\n","```\n","class Basemodel(nn.Module):\n","  \n","  def __init__(self,n_feature,n_hidden,n_output, keep_probab = 0.1):\n","    '''\n","    input : tensor of dimensions (batch_size*n_feature)\n","    output: tensor of dimension (batchsize*1)\n","\n","    num_features = 15 # of metrics\n","    num_hl = 128\n","    num_output = 1\n","    '''\n","    super().__init__()\n","  \n","    self.input_dim = n_feature    \n","    self.hidden = nn.Linear(n_feature, n_hidden) \n","    self.predict = torch.nn.Linear(n_hidden, n_output)\n","    self.dropout = nn.Dropout(keep_probab)\n","\n","  def forward(self, x):\n","    x = F.relu(self.dropout(self.hidden(x)))\n","    x = self.predict(x)\n","    return x\n","```\n"," And got the test loss (MAE): 0.645\n","\n"," Exploring not only the accuracy of the metrics but also their respective distributions:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"D3XGDrE1Wqps","colab":{}},"source":["df_para_z = df_para.copy()\n","df_para_z['Normal_Label'],df_para_z['Normal_Linear_Preds'],df_para_z['Normal_MLP_Preds'] = stats.zscore(df_para.label),stats.zscore(df_para.Predictions), stats.zscore(df_para[\"MLP predictions\"])\n","\n","errors = []\n","for col in df_para_z.columns:\n","    if 'label' not in col.lower():\n","        if \"Normal\" in col:\n","          diff = df_para_z['Normal_Label'] - df_para_z[col]\n","          diff.name = col\n","          errors.append(diff.abs())\n","        else:\n","          diff = df_para_z['label'] - df_para_z[col]\n","          diff.name = col\n","          errors.append(diff.abs())\n","error_df = pd.concat(errors, axis=1)\n","error_df.describe()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XEadvMOPyur3","colab_type":"code","colab":{}},"source":["fig = plt.figure(figsize=(20, 10))\n","ax = fig.add_subplot(111)\n","ax.hist(x=df_para_z.Normal_Label, bins=15, alpha=0.5, rwidth=0.85, label='Normal_Label')\n","ax.hist(x=df_para_z.Normal_Linear_Preds, bins=15, alpha=0.5, rwidth=0.85, label='Normal_Linear_Preds')\n","ax.hist(x=df_para_z.Normal_MLP_Preds, bins=15, alpha=0.5, rwidth=0.85, label='Normal_MLP_Preds')\n","plt.title('Histogram of Predicted Output vs Labels')\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"QkAl4Q-yRX-W"},"source":["What we see is that the (normalized) human labels follow a normal distribution (with a slight left skew, the (normalized) linear model predictions seems to be normalish with two peaks, and the (normalized) non-linear predictions follows a normal distribution also (just with a much smaller variance).\n","\n","\n","Just looking at the scores themselves, neither model does exceedingly well (one must take into account the severe noiseness of human-labeling - which we will discuss later on), however it does seem though that a non-linear model better captures the label distribution."]},{"cell_type":"markdown","metadata":{"id":"q3NReKaLyusD","colab_type":"text"},"source":["Below is an interactive plot to explore all of the distributions for all of the predictions and metrics"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"HcF_MiTROwhL","colab":{}},"source":["fig = go.Figure(layout_title_text=\"Histogram of Similarity Metric Scores\",)\n","for col in df_para_z.columns:\n","    fig.add_trace(go.Histogram(x=df_para_z[col], name=col, nbinsx=25))\n","\n","# Overlay both histograms\n","fig.update_layout(barmode='overlay', height=600)\n","# Reduce opacity to see both histograms\n","fig.update_traces(opacity=0.55)\n","fig.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"HQrexBuI0QaJ"},"source":["Below you can explore the distribution of errors for each metric, and we can see that there is significant variance and difference between them."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"tdjvzkuRX0Mw","colab":{}},"source":["fig = go.Figure(layout_title_text=\"Similarity Metric Errors Histogram\")\n","for col in error_df.columns:\n","    fig.add_trace(go.Histogram(x=error_df[col], name=col, nbinsx=50), )\n","\n","# Overlay both histograms\n","fig.update_layout(barmode='overlay')\n","# Reduce opacity to see both histograms\n","fig.update_traces(opacity=0.5)\n","fig.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"OuHKcD_20zZF"},"source":["What we also see is that our predictions (both linear and MLP), are within the variance of the annotations (+-1) which is relatively fair."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"QSvKLQ9yaEVP","colab":{}},"source":["plt.scatter(df_para_z.label,error_df.Normal_MLP_Preds, label=\"Normal_MLP_Error\")\n","plt.scatter(df_para_z.label,error_df.Normal_Linear_Preds, label=\"Normal_Linear_Error\")\n","plt.plot(df_para_z.label,[error_df.Normal_MLP_Preds.mean()] * df_para_z.label.shape[0], label=\"mlp_mean_error\")\n","plt.plot(df_para_z.label,[error_df.Normal_Linear_Preds.mean()] * df_para_z.label.shape[0], label=\"linear_mean_error\")\n","\n","plt.xlabel('label')\n","plt.ylabel('error')\n","plt.title('Error rate per labels')\n","plt.legend()\n","plt.gcf().set_size_inches(20, 10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"77BiAn3bdo1c"},"source":["While the MLP model has a more similar distribution to the human labeling, the linear model seems to do have an overall smaller error variance. ( Theoretically speaking with enough fine-tuning, we should be able to get the MLP model to match the linear model, if not beat it. This is just the exploratory phase for a POC)\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"G0GXn9-WhYW-","colab":{}},"source":["df_para['text_1'] = df_texts['text_1']\n","df_para['text_2'] = df_texts['text_2']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"agFnVSojgp11","colab":{}},"source":["high_err = df_para[np.abs(df_para.Predictions- df_para.label)>1]\n","low_err = df_para[np.abs(df_para.Predictions- df_para.label)<.25]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"iGjiqJSzgipN","colab":{}},"source":["high_samps, low_samps = np.random.choice(high_err.index,10),np.random.choice(low_err.index,10)\n","print(\"samples of high error sentences\")\n","for i in high_samps:\n","  print(f\"\\nsentence 1 ({i}): {df_para.iloc[i].text_1}\\n\" +\n","      f\"sentence 2:{df_para.iloc[i].text_2}\\n\" +\n","      f\"label:{np.round(df_para.iloc[i].label,2)} prediction:{np.round(df_para.iloc[i].Predictions,2)},\" +\n","      f\"difference: {np.round(df_para.iloc[i].Predictions- df_para.iloc[i].label,2)}\")\n","\n","print(\"\\n\\n\\nsamples of low error sentences\")\n","for i in low_samps:\n","  print(f\"\\nsentence 1: {df_para.iloc[i].text_1}\\n\" +\n","        f\"sentence 2:{df_para.iloc[i].text_2}\\n\" +\n","        f\"label:{np.round(df_para.iloc[i].label,2)} prediction:{np.round(df_para.iloc[i].Predictions,2)},\"+\n","        f\"difference: {np.round(df_para.iloc[i].Predictions- df_para.iloc[i].label,2)}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"tkkdhQMj2nv2"},"source":["Reflecting on anecdotal evidence, we can see examples where our predictions would actually be preferred to the human-label, as well as examples of where the human labeling didn't seem intuitive at all.\n","\n","While this is anecdotal, it strengthens the argument that the human labels are quite noisy and unreliable."]},{"cell_type":"markdown","metadata":{"id":"jgB_qUZuyutd","colab_type":"text"},"source":["### Analysis of All Datasets"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"AxYgMRXcZDMG"},"source":["We then wanted to see how well our models worked on other datasets (all together, and each separate).\n","\n","In both scenarios, we used the same architectures that we did for the Paraphrase dataset, but just train them on their respective datasets."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"iRLSyrQ9xwPL","colab":{}},"source":["df_all.head(3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0I_AJlHF6gz9"},"source":["The test loss (MAE) between the prediction and label for each dataset based off the MLP model:\n","\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"XPvp6P7i2D45","colab":{}},"source":["df_mlp_testloss = df_mlp_testloss.sort_values(by=\"Test Loss\").reset_index().drop(columns=['index'])\n","df_mlp_testloss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"4cejvgu5IpeB"},"source":["The test loss (MAE) between the prediction and label for each dataset based off the linear model:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"K0cWNRmKIeCK","colab":{}},"source":["df_linear_testloss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"8NeW8uRi-jBr"},"source":["Feature weights (from the Linear Model) trained on all the datasets:<br><br>\n","\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>feature</th>\n","      <th>importance</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>bleu_withoutstop</td>\n","      <td>0.905143</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>ftext_withoutstop</td>\n","      <td>0.094857</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>bleu_allwords</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>ROUGE-2 recall</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>L2_score</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>POS dist score</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>chrf_score_norm</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>chrf_score</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>ROUGE-L F</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>ROUGE-L precision</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>ROUGE-L recall</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>ROUGE-2 F</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>ROUGE-2 precision</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>ROUGE-1 precision</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>ROUGE-1 F</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>ROUGE-1 recall</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>4-gram_overlap</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>3-gram_overlap</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>2-gram_overlap</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>1-gram_overlap</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>WMD</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ftext_allwords</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>glove_withoutstop</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>glove_allwords</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>bert</td>\n","      <td>0.000000</td>\n","    </tr>\n","  </tbody>\n","</table>"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6zM3SGxSAHBt","colab":{}},"source":["with open(f'{root}/data/feature_weights_per_dataset.txt', 'r+') as f:\n","  txt = f.read()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"mFZr7jL1AMMj","colab":{}},"source":["all_scores =  txt.split(sep=\"********************************************************************\\n\")\n","new_scores = [scores.split(\"\\n\") for scores in all_scores if scores != '']\n","scores_dict = {}\n","for score in new_scores:\n","  title = re.sub(\"prediect labels for dataset \",\"\",score[0])\n","  _, mse_score = score[1].split(\": \")\n","  scores_dict[title] = {'mse': mse_score}\n","  for values in score[4:-1]:\n","    value = values.split(\"  \")\n","    value = [val.strip() for val in value if val != \"\"]   \n","    scores_dict[title][value[1]] = float(value[-1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"QPMqdhZlEBeF","colab":{}},"source":["pd.DataFrame.from_dict(scores_dict).T"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"vvLDwIaD4cYB","colab":{}},"source":["df_all['Normal_Label'],df_all['Normal_Linear_Preds'] = stats.zscore(df_all.label),stats.zscore(df_all.Predictions)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5r6NUqQHyUUm","colab":{}},"source":["cmap = {ds:i for i, ds in enumerate(df_all.dataset.unique())}\n","df_all['abs_diff'],df_all['abs_zdiff'] = np.abs(df_all.Predictions - df_all.label),np.abs(stats.zscore(df_all.Predictions - df_all.label))\n","\n","\n","fig, ax = plt.subplots()\n","\n","scatter = ax.scatter(df_all.label,df_all.Predictions, c = [cmap[d] for d in df_all.dataset])\n","plt.gcf().set_size_inches(20, 10)\n","plt.xlabel('label')\n","plt.ylabel('predictions')\n","plt.title('predictions VS actual labels')\n","\n","legend1 = ax.legend(*scatter.legend_elements(),\n","                    loc=\"best\", title=\"datasets\")\n","ax.add_artist(legend1)\n","plt.show();"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"dHBGmbud4O3Q","colab":{}},"source":["f, ax = plt.subplots(8,4,figsize = (40,40))\n","for i,s in enumerate(df_all.dataset.unique()):\n","  ax[i%5] = plt.subplot(8,4,i+1) \n","  ax[i%5] = plt.scatter(df_all[df_all.dataset==s].label,df_all[df_all.dataset==s].Predictions)\n","  plt.xlabel('labels')\n","  plt.ylabel('predictions')\n","  plt.title(s)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"cAD2e0OH7rBE"},"source":["While the ensemble works for some datasets, however many other datasets, the idea of using an ensemble method doesn't work at all - and our results look like noise. This could be due to the lack of labels for the dataset or that the dataset itself sentences are too ambigous. Exploring which datasets work better would also help us develop better heuristics (part 2)."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Ds-3t4Bd6XTx","colab":{}},"source":["high_err, low_err = df_all[df_all.abs_zdiff>1],df_all[df_all.abs_zdiff<.25]\n","high_err = high_err.drop(['text_1_tokens','text_2_tokens'],axis=1)\n","low_err = low_err.drop(['text_1_tokens','text_2_tokens'],axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"-0v-WsE86an0","tags":[],"colab":{}},"source":["high_samps, low_samps = np.random.choice(high_err.index,10),np.random.choice(low_err.index,10)\n","print(\"samples of high error sentences\")\n","for i in high_samps:\n","  print(f\"\\ndataset: {high_err.dataset[i]}\\nsentence 1: {high_err.text_1[i]}\\nsentence 2:{high_err.text_2[i]}\\nlabel:{np.round(high_err.label[i],2)} prediction:{np.round(high_err.Predictions[i],2)}, difference: {np.round(high_err.Predictions[i]- high_err.label[i],2)}\")\n","\n","print(\"\\n\\n\\nsamples of low error sentences\")\n","for i in low_samps:\n","  print(f\"\\ndataset: {low_err.dataset[i]}\\nsentence 1: {low_err.text_1[i]}\\nsentence 2:{low_err.text_2[i]}\\nlabel:{np.round(low_err.label[i],2)} prediction:{np.round(low_err.Predictions[i],2)}, difference: {np.round(low_err.Predictions[i]- low_err.label[i],2)}\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"z5DRzR0k8r0u"},"source":["1. We need to tackle the nosiness of the human labeled data. We offer a suggestion in part 2.\n","2. While non-linear model distributions was similar to the human labeling, we saw that they didn't perform exceedingly well, and were even less beneficial when we applied it to other datasets.\n","3. While using the ensemble method works for some datasets, in other datasets (or a generic ensemble) doesn't perform well at all."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_woEBT16911c"},"source":["## The Theoretical Element"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9xxPM42REfv8"},"source":["What we need to do is to hypothesize what are the underlying heuristics beneath perceived semantic similairty. \n","\n","We would then develop particular pairs of sentences in which the distinction would be on that given heuristic, and see how it impacts the human score.<br> \n","\n","Ultimately, instead of having one human-labeled metric for semantic similarity, we would have a human-labeled score for each of the underlying heuristic. The current ideas for heuristics are:<br>\n","1. Word Overlap\n","2. Word similarity \n","3. Subject-Object relationship \n","4. Type of sentence (Question/Statement/Elaboration/etc.)\n","5. Mood\n","6. Sentiment Similarity (Could be seen as a subdivision of Mood)\n","7. Similar sentence length.\n","8. Bias within size of the sentences \n","9. Grammatical consistency (same form of mistakes, etc.)\n","10. Spelling/Writing dialect<br>\n","\n","As we get a better understand of the underlying features, we may be also are to come back to our current metrics with a deeper understanding of what exactly they are capturing.\n","\n","We should also establish clearer guidelines as to what form of human labeling we accept. Being that they are still asked to reflect on their subjective evaluation, we need a way to measure whether or not humans could give us a good approximation for any particular pair. This can be encouraged by only taking labels where there is a form of **consensus** between a majority of all labelers (ex: where the variance of the label within a given range is within a certain range.)\n","\n","With a richer language to discuss semantic similarity, we can look deeper into style transfer and paragraph generation and be able to ask, what underlying heuristics are we actually looking for in a given task. \n","\n","While we have not fully explored every possible avenue of curiosity, we hope we have at least provided you with a promising idea for future research. We look at our exploration as 'laying the ground-work' as opposed to 'defending a thesis'. \n","\n","To quote the great Geoffrey Hinton: \"To deal with a 14-dimensional space, visualize a 3-D space and say 'fourteen' to yourself very loudly. Everyone does it.\""]}]}