{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled27.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMHcbz4dsLIGGaB3eH6sWzM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hernanros/SOTA/blob/master/docs/Meetings%20and%20communication/meetings%20with%20Ivan%20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MykvfxUCjh3R",
        "colab_type": "text"
      },
      "source": [
        "## Meeting with Ivan 18.08.2020\n",
        "\n",
        "\n",
        "option 1:<br>\n",
        "adding our work to another paper that is currently an autherization process\n",
        "\n",
        "option 2: <br>\n",
        "Working on an original papers\n",
        "\n",
        "## Semantic Similarity\n",
        "\n",
        "### Where do you think the potential for our work?\n",
        "1. aspect 1: different sentences get different sematic similarity score based on the proporties of the text\n",
        "2. aspect 2: different people make different estimates of similarity\n",
        "\n",
        "### data collection\n",
        "- controlling for the ID of the raters to get a normalized similarity scores\n",
        "\n",
        "### Ideas to explore (Ivan):\n",
        "upload data with Id so we can control for individual users\n",
        "- use negative examples to create a user segmentation based on a base-dissimilarity \n",
        "- look into varience of human ratings on given phrases\n",
        "- length of the sentence\n",
        "\n",
        "- Purpose our similarity metric and evaluate on existing metrics\n",
        "\n",
        "\n",
        "### Next steps:\n",
        "1. use the existing metric we developed on an unseen data to try predict human labels. according to these results \n",
        "\n",
        "2. normalize human labels according to negative pairs (random sentences) as a baseline for sentences of un similar sentences.\n",
        "Also calculate normalized scores per labeler\n",
        "\n",
        "3. cluster labelers based on scoring patterns in the negative samples\n",
        "\n",
        "4. measure sub-components of similarity (will involve collection of new data) and check the correlations of different metrics with those scores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DRj1c07vI9S",
        "colab_type": "text"
      },
      "source": [
        "# Missions\n",
        "- Re-read Ivan's paper that we based on (EVERYONE)\n",
        "- Explopre the new data (especially the labels and the difference between \"real\" and \"negative\" pairs (Hernan & ....)\n",
        "- Run the existing MLP Model on the ppq data set (Shaul)\n",
        "- refactor the git (Hezi)\n",
        "- translate notebooks into py files for a project pipeline. (Hezi & Adam)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4O6M8hqmbs2",
        "colab_type": "text"
      },
      "source": [
        "# Meeting with Ivan 25/08/2020\n",
        "\n",
        "## EDA\n",
        "To check:\n",
        "- correlation between the length of the sentences and the duration of the task\n",
        "- the relation between the tendency of labelers to label more extremly on both dataset (as labelers give lower rating on the random pairs, they should trend to 5 on non-random pairs)\n",
        "- explore the user ratings values\n",
        "- filter out labelers with higher mean than 2 on random pairs\n",
        "- calculate the correlation between the scores and labels on the filtered data\n",
        "\n",
        "\n",
        "### For next week:\n",
        "- baseline on length and difference between random and non-random\n",
        "- exploration of the difference between extremety of labelers\n",
        "- one click pipeline automation\n",
        "- list of hypothesis that we want to check wrt the dataset.\n",
        "\n",
        "## Dates and Deadlines:\n",
        "- 28.09.2020 - deadline for submission for [ICLR](https://www.iclr.cc/Conferences/2021/Dates)\n",
        "until the 20.09 we need to decide if we are planning to publish on ICLR and calculate our next steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZTgJ7dlxrxQ",
        "colab_type": "text"
      },
      "source": [
        "# Possible Hypothesis\n",
        "1. Removing noise (bad labelers) would improve pur existing models\n",
        "- labelers that give random values to all pairs\n",
        "- mean for random labels higher than mean for non-random labels\n",
        "- random varience larger than non-random vairence\n",
        "- low correlation between mission duration and length of sentences\n",
        "\n",
        "2. we might find different clusters of types of labelers:\n",
        "- one possible division might bw the division between radicals and centralist: \n",
        "  - transforming labels to binary scale might create a more standardised results that remove some of the varience that arise from such a distinction.\n",
        "  - check for consistency in how labelers label different pairs\n",
        "\n",
        "we might want to adress each group seperatly **OR** we will might want to normalize\n",
        "\n",
        "3. identifying pairs that evoke very different labels (among good labelers)\n",
        "  - assess if we have a consuencus score for the pairs\n",
        "according to the original Hypothesis, pairs that have high varience, might prove that different labelers interpert semantic similarity according to different features of similarity. when the varience is high we should see incongruiety between the different scoring metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw7f6Kttq726",
        "colab_type": "text"
      },
      "source": [
        "# Meeting with Ivan 01/09/2020\n",
        "\n",
        "- good semantic similarity should be more percise when sentence are similar but, less percise for sentences that are far apart (log scale)\n",
        "  - the varience of pairs with high mean than varience of pairs of sentences with low mean beacuse humans are wired detecting same patterns, so there should e a higher consensus on similar sentences.\n",
        "- We could use WMD/POS distance to weight labels on a hyperbolic space (that penlizes distance on non linear scaling)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USVWrfNJ1cBR",
        "colab_type": "text"
      },
      "source": [
        "# For next week:\n",
        "- combine hueristics of Shaul's and Hernan's notebooks (duration outliers and mean gap between random & non random pairs)\n",
        "- seperate extremists and centralist  annotators\n",
        "- exploring difference in labeling according to different sentiments:\n",
        "  - (a) take each sentences pairs:\n",
        "  - (b) use existing packages to score each sentence sentiment\n",
        "  - (c) score sentiment difference for each pair\n",
        "  - (d) look for correlations between labels across different sentiment groups\n",
        "  - (e) see if we can break down labeling strategies conditioned on sentiment both within and between labelers\n",
        "\n",
        "## for later on:\n",
        "- re-run scoring, model training and result analysis on new dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHtlMg46yKSN",
        "colab_type": "text"
      },
      "source": [
        "# Weekly meeting 08.09.2020\n",
        "\n",
        "## weekly rundown\n",
        "#### labelers filtering\n",
        "- combined filtering methods from last week\n",
        "\n",
        "#### sentiment analysis\n",
        "- identified sentences pairs where most of the words are the same (high BLEU score) and explored the extremity measured on those types of sentences, to identify the affect of negation on semantic interpetation\n",
        "\n",
        "\n",
        "## For next week\n",
        "- set up a new repository with a logical structure\n",
        "- filter out annotators that have high fluctuation in labeleing stratagy for very high BLEU score\n",
        "- produce analysis on the filtered dataset\n",
        "- WMD exploration with log scaling\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tY-ezZi632pA",
        "colab_type": "text"
      },
      "source": [
        "# Weekly meeting 15.09.2020\n",
        "- calculate all similarity metrics\n",
        "- calculate scores seperatly for users that have mean label for *random* pairs lowe than 1.5 and higher than 1.5\n",
        "  - cross check the scoring for centralists and extremists\n",
        "- seperate calculation for sentences that we have more than 1 score for\n",
        "-calculate correlation for each sub group between the labels and algorithm scores "
      ]
    },
    {
      "source": [
        "# Weekly meeting 06.10.2020\n",
        "- Ivan (WMD metircs) - expirementing with non linear scaling function. the current problem is that we can't come up with a good mathematical explanation.\n",
        "we want to come up with a function where the distances between tokens is sensitive for small changes and lower sensitivity for bigger changes \n",
        "- correlation exploration for different groups of labelers (combined vs. filtered)\n",
        "\n",
        "# For next week\n",
        "- litereture exploration on labeler research related to NLP and machine learning. (Quality control of annotators)\n",
        "- Run correlations between labels and metrics for:\n",
        "    - combined dataset\n",
        "    - filtered dataset extremist VS. centralist\n",
        "- train RF and MLP models with metrics (and some metadata) as feutures.\n",
        "\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "iRW6wPWy4KYR",
        "colab_type": "code",
        "colab": {}
      }
    }
  ]
}