{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SOTA group meetings.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hernanros/SOTA/blob/master/Meetings%20and%20communication/SOTA_group_meetings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-VqT9cmtDt0",
        "colab_type": "text"
      },
      "source": [
        "# NLP group meeting 07.06.2020\n",
        "Hernan, Shaul, Lee, Adam, Hezi\n",
        "\n",
        "### exploring non linearities between different components of the term semantics\n",
        "  - Theoretical frame work:\n",
        "  trying to break down the term semantics to sub-components based on existing research\n",
        "  - learning non-linear relations \n",
        "\n",
        "1. get different similarities scores between the sentences in the datasets\n",
        "2. try to use NN's with the different similarities scores as features and the Human similarities scores as labels\n",
        "3. exploring linguistic research to break down Sematics into different sub-terms \n",
        "\n",
        "**Notes**\n",
        "- should we take all simlaritiy metrics?\n",
        "- We're all starting with the same dataset: paraphrase.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6L7aGzd1v9IC",
        "colab_type": "text"
      },
      "source": [
        "# Work division\n",
        "## Similarities calculation\n",
        "- implementation pipeline for each similarity metric might be a bit complicated\n",
        "we will want to split the coding implemintation to groups based on the mutual feature of each metric: <br>\n",
        " - group 1 (embedding based similarity scores):<br>\n",
        "POS distance (Lee), cosine similarities(Glove, fast-text) (Hezi) , L2 (elmo based)(Adam), WMD (Hernanm), BERT(Shaul)\n",
        "\n",
        " - group 2 (n-gram based scores):<br>\n",
        "word-overlap (Hernan), chr-f(Shaul), BLEU(Hezi), ROUGE(Lee) \n",
        "\n",
        "## Tehoretical background research\n",
        "\n",
        "## NN Implemintation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DeJOnFctDv9",
        "colab_type": "text"
      },
      "source": [
        "# Useful links\n",
        "https://pypi.org/project/bert-score/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvXoQsiZtDyn",
        "colab_type": "text"
      },
      "source": [
        "# SOTA group meeting 14.06\n",
        "\n",
        "### What was done from last week?\n",
        "- 9 of the similarities calculated\n",
        "- calculation notebook added to the git repository\n",
        "\n",
        "\n",
        "### possible issues\n",
        "- unconstintansy in tokenization in the different packages implemantation\n",
        "- \n",
        "\n",
        "### what's next?\n",
        "- consider adding AlBert similarity score (Hezi)\n",
        "- try to think of ideas of spliting the concept of semantics into sub-parts (e.g identical sentences that one of them has one negation word, similar sentiment and idea with different style, etc.)\n",
        "- finish pushing the labels to the git<br> \n",
        "\n",
        "\n",
        "seperate into 2 teams:\n",
        "1. Basic EDA (histograms, correlations, identify consensus sentences), simple regression models to predict the labels (linear regression\\ random forrest) and explore the weights and preform error analysis. (Hernan, Adam)<br>\n",
        "\n",
        "2. MLP netweork to explore the connection between the different features (similarity scores) - design a relatively simple architechture, pick loss functions (most likely MSE or MAE). preform error analysis on the network results and weight exploration. (Shaul, Lee)<br>\n",
        "\n",
        "3. Hezi is caluclating new and exsiting similarity measures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoMeBuqCCMUs",
        "colab_type": "text"
      },
      "source": [
        "# SOTA group meeting 21.06\n",
        "## What was done Since last week?\n",
        "- MLP NN:<br>\n",
        "implemented 2: basic NN network with all similarity scores as features\n",
        "  - network 1 - 1 hidden layer (128), all fully connected, dropout layer (negligable), Relu activation,10 epochs, MAE loss. test loss - .66  \n",
        "\n",
        "  - network 2 - 2 hidden layers (50, 100),  MSE loss, Adam optimizer, no dropout, 200 epochs, .25 test size, leaky Relu, MSE loss. test loss - 0.63\n",
        "\n",
        "- Classic ML regressor<br>\n",
        "no additional work was done \n",
        "\n",
        "## what to do next:\n",
        "1. run the linear model for the paraphrase dataset (Lee)\n",
        "2. create a total combined scores matrix for **ALL** Datasets (add identfication for each dataset) (Hezi & Hernan)\n",
        "3. error analysis on the preformance of the MLP network and linear regression. (Hernan / Adam)\n",
        "4. summary report (Shaul)\n",
        "5. try to think on interesting expirements to break down sematics into pieces.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2s2MOJg86ZWr",
        "colab_type": "text"
      },
      "source": [
        "# 02.07 Meeting\n",
        "###What was done this week?\n",
        "1. run the linear model on the combimed dataset as a whole and on \n",
        "2. result exploration on the combined datasets with the linear model\n",
        "\n",
        "From the result exploration we can see that there is a great amount of noise with the labels themselves. we want to stress the point that we could dance just fine. is the floor the one whose crooked.<br>\n",
        "\n",
        "We found that in some of the datasets the similarity methods that we have do manage to learn enough to give similar scores as the human annotators\n",
        "\n",
        "- We might use consensus scores (where the label is a round number) as a metric to define \"good labeling\", if we find high correlation between labels and predictions on those labels, that is an evidence that when labels are done properly, our model works.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}